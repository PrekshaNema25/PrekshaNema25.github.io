<head>
<title> Preksha Nema </title>

</head>
# About Me
<div style = "text-align: justify"> I am currently pursuing my Ph.D. under Dr. Mitesh M. Khapra and co-guided by Dr. Balaraman Ravindran at IIT Madras. I joined IIT Madras as an M.Tech in July 2015, and then converted to Ph.D. programme in March 2017. My area of research is Deep learning for NLP. Currently my work is focussed on modelling better attention mechanism techniques for different Natural Language Generation tasks.
</div>

# Awards
Google India Ph.D. Fellowship, 2017

# Publications:
<ul>
<li> <h3>Move On And Never Look Back: An Improved Neural Encoder Decoder Architecture for Generating Natural Language Descriptions from Structured Data</h3><font color="grey"><h5><i><b>Under review in TACL</b></i></h5></font>
   <font color="black"><h5> Preksha Nema, Shreyas Shetty, Parag Jain, Mitesh Khapra, Anirban Laha, Karthik Sankaranarayanan, Balaraman Ravindran</h5></font>
<table width="100%" align="center" border="0" cellspacing="0">
    <tr>
      <td width="30%">
      <img src='/images/nlb.jpg'>         
      </td>
      <td valign="top" width="70%"> 
        <div style = "text-align: justify"> <h6> Neural models for Natural language generation (NLG) have been successfully used in a wide variety of tasks such as machine translation, summarization, dialog generation, etc. In this work, we focus on one such NLG task which involves rendering natural language descriptions from a structured table of facts. A typical example of such a task is generating product descriptions from structured catalog tables. Unlike other NLG tasks the input here has a specific structure which can be exploited for building richer neural models. We propose such models which take cognizance of the : (i) hierarchical organization of facts (ii) need for continuous attention on a fact for multiple time steps and (iii) need for ignoring facts which have already been considered. We experiment with a recently released dataset which contains fact tables about people and their corresponding one line biographical descriptions in English. In addition, we also introduce two similar datasets for French and German. Our experiments show that the proposed model gives 21% relative improvement over a recently proposed state of the art method and 10% relative improvement over basic seq2seq models..</h6></div></td>
   </tr>
   </table>
   </li>
   
<li> <a href="https://arxiv.org/abs/1704.08300"><h3>Diversity driven Attention Model for Query-based Abstractive Summarization</h3></a><font color="grey"><h5><i><b> Association of Computational Linguistics (ACL), 2017</b></i></h5></font>
   <font color="black"><h5> Preksha Nema, Mitesh M. Khapra, Anirban Laha, Balaraman Ravindran </h5></font>
<table width="100%" align="center" border="0" cellspacing="0">
    <tr>
      <td width="30%">
      <img src='/images/query.png'>         
      </td>
      <td valign="top" width="70%"> 
        <div style = "text-align: justify"> <h6> Abstractive summarization aims to generate a shorter version of the document covering all the salient points in a compact and coherent fashion. On the other hand, query-based summarization highlights those points that are relevant in the context of a given query. The encode-attend-decode paradigm has achieved notable success in machine translation, extractive summarization, dialog systems, etc. But it suffers from the drawback of generation of repeated phrases. In this work we propose a model for the query-based summarization task based on the encode-attend-decode paradigm with two key additions (i) a query attention model (in addition to document attention model) which learns to focus on different portions of the query at different time steps (instead of using a static representation for the query) and (ii) a new diversity based attention model which aims to alleviate the problem of repeating phrases in the summary. In order to enable the testing of this model we introduce a new query-based summarization dataset building on debatepedia. Our experiments show that with these two additions the proposed model clearly outperforms vanilla encode-attend-decode models with a gain of 28\% (absolute) in ROUGE-L scores.</h6></div>
        <ul>
        <li><a href="https://github.com/PrekshaNema25/diversity_based_attention"> [Data + Code] </a></li>
        </ul></td></tr>
   </table>
   </li></ul>
   
# Work Experience
I worked in Nvidia Graphics Pvt. Ltd. from June 2012 - June 2015 as a System Software Engineer in Resource Manager- Professional Soultions Group team.

# Academic Details
I have completed my B.Tech from Visvesvaraya National Institute of Technology, Nagpur in Computer Science and Engineering in 2012.

# Contact:

[Resume](pdfs/resume.pdf)

preksha [at] cse[dot]iitm[dot]ac[dot]in
